{
  "metadata": {
    "name": "Reddit Data cleaning and profiling",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data cleaning and profiling for reddit post dataset\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "First we read the csv file. Since the column of body contains text information, `multiLine` is necessary to avoid a mess in format."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\nval filePath \u003d \"bitcoin_reddit_all.csv\"\nval rawDF \u003d spark.read.option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"multiLine\", \"true\")\n    .option(\"escape\", \"\\\"\")\n    .csv(filePath)\n    \nrawDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "rawDF.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Checking on errors and formats\n\nSince there are several different columns of time, we don\u0027t have to drow the data if one of record is missing.\nAlso, checking for null values before dropping duplicated columns helps me reduce data loss.\n\n\nHere, we found that this dataset is already cleaned. All records have its own id, date, time and unix timestamp, we can simply start data filtering and profiling in the next step."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val errorRows \u003d rawDF.filter(col(\"_c0\").isNull)\n errorRows.cache().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val errorRows \u003d rawDF.filter(col(\"datetime\").isNull \u0026\u0026 col(\"date\").isNotNull)\n errorRows.cache().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val errorRows \u003d rawDF.filter(col(\"date\").isNull \u0026\u0026 col(\"datetime\").isNotNull)\n errorRows.cache().count()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val errorRows \u003d rawDF.filter(col(\"body\").isNull)\n errorRows.cache().count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We start from selecting useful information. \n\n | Column Name| Description |\n | :----------: | :----------: |  \n | _c0 | Id |\n | datetime | Timestamp |\n | score:| Upvotes or Downvotes |\n | controversiality | Controversiality |\n | body | Comment text |"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nval baseDF \u003d rawDF.select(\n            \"_c0\",\n            \"datetime\",\n            \"score\",\n            \"controversiality\",\n            \"body\")\nbaseDF.cache().count"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "baseDF.columns"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Filtering data according to timestamp\n\nCurrently the start and end time is set to include all the data. Since We need to join other datasets later in this project, we will need to find the data within the same time range. \n\nIn this step, we convert datatime into the dype of timestamp and select them within given range."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions.to_timestamp\nval dfWithTimestamp \u003d baseDF.withColumn(\"timestamp\", to_timestamp(col(\"datetime\"), \"yyyy-MM-dd HH:mm:ss\")).drop(\"datetime\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(dfWithTimestamp)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfWithTimestamp.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val startTime \u003d \"2009-1-1 00:00:00\"\nval endTime \u003d \"2019-12-30 00:00:00\"\nval filteredDateDF \u003d dfWithTimestamp.filter(col(\"timestamp\").between(startTime, endTime))"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(filteredDateDF)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Body message cleanning\n\nLooking at the first few comments, it definetly appears we will need to do some processing on the text to clean it up before pushing through a sentiment analyzer.\n\nA couple of items we should be sure to handle:\n- urls\n- special characters\n- new lines\n- foreign languages \n- numbers"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val cleanedBodyDF \u003d filteredDateDF\n  .withColumn(\"body\", regexp_replace($\"body\", \"[^\\\\w\\\\s.?!]\", \"\")) // Remove special characters\n  .withColumn(\"body\", regexp_replace($\"body\", \"\\\\d\", \"\")) // Remove digits\n  .withColumn(\"body\", regexp_replace($\"body\", \"\\\\n\", \" \")) // Replace new lines with space\n  .withColumn(\"body\", regexp_replace($\"body\", \"http.*\\\\s\", \" \")) // Remove URLs"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "cleanedBodyDF.cache().count\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(cleanedBodyDF.select(\"body\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Text Body Sentiment\n\nCurrently untouched since we need to keep the sentiment model of two different dataset the same."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data profiling \n\nDescrible this dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Year \u0026 Count profiling"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions.{min, max}\n\nval TimestampStatsDF \u003d cleanedBodyDF.agg(\n  min($\"timestamp\").alias(\"Min Date\"),\n  max($\"timestamp\").alias(\"Max Date\")\n)\n\nz.show(TimestampStatsDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions.{year, month, dayofweek}\n\n// Example: Count of records per year\nval YearDistributionDF \u003d  cleanedBodyDF.groupBy(year($\"timestamp\").alias(\"Year\")).count().orderBy(\"Year\")\n\nz.show(YearDistributionDF)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This dataset contians posts on reddit related to Bitcoin from 2009-5-8 to 2019-12-29.\n\nThe number of posts grows fast in 2011 and keeps increasing in the next several years. In year 2017, it reaches the highest point and then decreases in the next two years. This might indicate the popularity of Bitcoin. "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Score profiling\n\nAfter sentiment, the score of posts with different attitute and year can be described in detail."
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(cleanedBodyDF.select(\"score\").describe())\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Controversiality profiling"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val avgControversialityDf \u003d dateDF.groupBy(year($\"timestamp\")).agg(mean(\"controversiality\").alias(\"avg_controversiality\"))\n\nz.show(avgControversialityDf)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dateDF \u003d cleanedBodyDF.withColumn(\"date\", date_format($\"timestamp\", \"yyyy-MM\"))\nval avgControversialityDf \u003d dateDF.groupBy(\"date\").agg(mean(\"controversiality\").alias(\"avg_controversiality\"))\n\nz.show(avgControversialityDf)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Obviously, a sharp increase of controversiality of this topic appears in 2012. Then, it maintains at a relatively low level. Which might match the record of price change and twitter change of Bitcoin in 2012."
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"bitcoin_reddit_clean.csv\"\n\ncleanedBodyDF.write.option(\"header\", \"true\") .mode(\"overwrite\").csv(outputPath)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val shortDF \u003d cleanedBodyDF.limit(50)\nshortDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPathShort \u003d \"bitcoin_reddit_clean_short.csv\"\n\ncleanedBodyDF.write.option(\"header\", \"true\") .mode(\"overwrite\").csv(outputPathShort)"
    }
  ]
}