{
 "metadata": {
  "name": "project_profiling",
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val filePath = \"bitcoin.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val rawDF = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"escape\", \"\\\"\")\n",
    "  .csv(filePath)\n",
    "\n",
    "z.show(rawDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Column Description Infer\n",
    "First I convert the column name to more readable names since the database lack column description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df1 = rawDF.withColumnRenamed(\"timestamp\", \"Index\")\n",
    "z.show(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{min, max}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val Row(minValue: Double, maxValue: Double) = df1.agg(min(\"Close\"), max(\"Close\")).head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can infer the currency is US dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df2 = df1.withColumnRenamed(\"Volume_(BTC)\", \"btc_volume\").withColumnRenamed(\"Volume_(Currency)\", \"usd_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### 2. Datatype check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type is fine. The 'infer' option has already convert the String type timestamp to timestamp data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. statistical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df2.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val imputeCols = Array(\n",
    "  \"Open\",\n",
    "  \"High\",\n",
    "  \"Low\", \n",
    "  \"Close\",\n",
    "  \"btc_volume\",\n",
    "  \"usd_volume\",\n",
    "  \"Weighted_Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setStrategy(\"median\")\n",
    "  .setInputCols(imputeCols)\n",
    "  .setOutputCols(imputeCols)\n",
    "  \n",
    "val imputedDF = imputer.fit(df2).transform(df2)\n",
    "\n",
    "z.show(imputedDF.describe())\n",
    "z.show(imputedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Getting rid of extreme or abnormal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "imputedDF.filter($\"Weighted_Price\" <= 0).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val valueDF = imputedDF.withColumn(\"PctChange\",($\"Close\"- $\"Open\")/$\"Open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(valueDF)\n",
    "z.show(valueDF.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of price change with one minute is reasonable. \n",
    "\n",
    "At most 41% increase and less than 100% decrease. Standard deviation is 0.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val w = Window.orderBy(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df3 = imputedDF.withColumn(\"return\", (col(\"Close\") - lag(\"Close\", 1).over(w)) / lag(\"close\", 1).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df3.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df4 = df3.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df5 = df4.withColumn(\"Volatility\", $\"High\"-$\"Low\")\n",
    "z.show(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "z.show(df5.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val outputPath = \"bitcoin-clean.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df5.write.mode(\"overwrite\").parquet(outputPath)"
   ]
  }
 ]
}
